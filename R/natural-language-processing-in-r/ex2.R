
library(tidyverse)
library(tidytext)
library(SnowballC)
library(tm)
library(widyr)
'Explore an R corpus
One of your coworkers has prepared a corpus of 
20 documents discussing crude oil, named crude. 
This is only a sample of several thousand articles 
you will receive next week. In order to get ready for 
running text analysis on these documents, you have decided
to explore their content and metadata.
Remember that in R, a VCorpus contains both meta 
and content regarding each text. In this lesson, you will explore these two objects.'


# Print out the corpus
data(crude)


# Print the content of the 10th article
crude[[10]]$content

# Find the first ID
crude[[1]]$meta$id

# Make a vector of IDs
ids <- c()
for(i in c(1:20)){
    ids <- append(ids, crude[[i]]$meta$id)
}


"Creating a tibble from a corpus
To further explore the corpus on crude oil
data that you received from a coworker, you have 
decided to create a pipeline to clean the text contained 
in the documents. Instead of exploring how to do this with 
the tm package, you have decided to transform the corpus into
a tibble so you can use the functions unnest_tokens(), count(),
and anti_join() that you are already familiar with. 
The corpus crude contains both the metadata and the text of each document."




# Create a tibble & Review
crude_tibble <- tidy(crude)
names(crude_tibble)

crude_counts <- crude_tibble %>%
    # Tokenize by word 
    unnest_tokens(word, text) %>%
    # Count by word
    count(word, sort = TRUE) %>%
    # Remove stop words
    anti_join(stop_words)


'Creating a corpus
You have created a tibble called russian_tweets that
contains around 20,000 tweets auto generated by bots
during the 2016 U.S. election cycle so that you can 
preform text analysis. However, when searching through 
the available options for performing the analysis you have 
chosen to do, you believe that the tm package offers the 
easiest path forward. In order to conduct the analysis, 
you first must create a corpus and attach potentially useful metadata.

Be aware that this is real data from Twitter and as such there is always 
a risk that it may contain profanity or other offensive content 
(in this exercise, and any following exercises that also use real Twitter data).'


# Create a corpus
russian_tweets <- read_csv("russian_1.csv")
tweet_corpus <- VCorpus(VectorSource(russian_tweets$content))

# Attach following and followers
meta(tweet_corpus, 'following') <- russian_tweets$following
meta(tweet_corpus, 'followers') <- russian_tweets$followers

# Review the meta data
head(meta(tweet_corpus))


'BoW Example
In literature reviews, researchers read and summarize as many 
available texts about a subject as possible. Sometimes they end up 
reading duplicate articles, or summaries of articles they have already read.
You have been given 20 articles about crude oil as an R object named crude_tibble.
Instead of jumping straight to reading each article, you have decided to see what
words are shared across these articles. To do so, you will start by building a
bag-of-words representation of the text.'

# Count occurrence by article_id and word
data.table::setnames(crude_tibble, "id", "article_id")
words <- crude_tibble %>%
    unnest_tokens(output = "word", token = "words", input = text) %>%
    anti_join(stop_words) %>%
    count(article_id, word, sort=TRUE)

# How many different word/article combinations are there?
unique_combinations <- nrow(words)

# Filter to responses with the word "prices"
words_with_prices <- words %>%
    filter(word == "prices")

# How many articles had the word "prices"?
number_of_price_articles <- nrow(words_with_prices)


'Sparse matrices
During the video lesson you learned about sparse matrices. 
Sparse matrices can become computational nightmares as the
number of text documents and the number of unique words grow. 
Creating word representations with tweets can easily create 
sparse matrices because emojis, slang, acronyms, and other forms of language are used.
In this exercise you will walk through the steps to calculate how sparse the Russian tweet dataset is.
Note that this is a small example of how quickly text analysis can become a major computational problem.'


# Tokenize and remove stop words
tidy_tweets <- russian_tweets %>%
  unnest_tokens(word, content) %>%
  anti_join(stop_words)


# Count by word
unique_words <- tidy_tweets %>%
  count(word)

# Count by tweet (tweet_id) and word
unique_words_by_tweet <- tidy_tweets %>%
  count(tweet_id, word)

# Find the size of matrix
size <- nrow(russian_tweets) * nrow(unique_words)
# Find percent of entries that would have a value
percent <- nrow(unique_words_by_tweet ) / size
percent



'TFIDF Practice
Earlier you looked at a bag-of-words representation 
of articles on crude oil. Calculating TFIDF values 
relies on this bag-of-words representation, but takes into 
account how often a word appears in an article, and how often
that word appears in the collection of articles.
To determine how meaningful words would be when comparing 
different articles, calculate the TFIDF weights for the words in crude, 
a collection of 20 articles about crude oil.'


# Create a tibble with TFIDF values
crude_weights <- crude_tibble %>%
  unnest_tokens(output = "word", token = "words", input = text) %>%
  anti_join(stop_words) %>%
  count(article_id, word) %>%
  bind_tf_idf(word, article_id, n)

# Find the highest TFIDF values
crude_weights %>%
  arrange(desc(tf_idf))

# Find the lowest non-zero TFIDF values
crude_weights %>%
  filter(tf_idf != 0) %>%
  arrange(tf_idf)


'An example of failing at text analysis
Early on, you discussed the power of removing stop 
words before conducting text analysis. In this most
recent chapter, you reviewed using cosine similarity to 
identify texts that are similar to each other.

In this exercise, you will explore the very real
possibility of failing to use text analysis properly. You will
compute cosine similarities for the chapters in the book Animal Farm,
without removing stop-words.'

# Create word counts
animal_farm_counts <- animal_farm %>%
  unnest_tokens(word, text_column) %>%
  count(chapter, word)

# Calculate the cosine similarity by chapter, using words
comparisons <- animal_farm_counts %>%
  pairwise_similarity(chapter, word, n) %>%
  arrange(desc(similarity))

# Print the mean of the similarity values
comparisons %>%
  summarize(mean = mean(similarity))



'Cosine similarity example
The plot of Animal Farm is pretty simple. In the beginning the 
animals are unhappy with following their human leaders. In the 
middle they overthrow those leaders, and in the end they become unhappy 
with the animals that eventually became their new leaders.

If done correctly, cosine similarity can help identify documents 
(chapters) that are similar to each other. In this exercise, you will
identify similar chapters in Animal Farm. Odds are, chapter 1 (the beginning) 
and chapter 10 (the end) will be similar.'

# Create word counts 
animal_farm_counts <- animal_farm %>%
  unnest_tokens(word, text_column) %>%
  anti_join(stop_words) %>%
  count(chapter, word) %>%
  bind_tf_idf(chapter, word, n)

# Calculate cosine similarity on word counts
animal_farm_counts %>%
  pairwise_similarity(chapter, word, n) %>%
  arrange(desc(similarity))
# Calculate cosine similarity using tf_idf values
animal_farm_counts %>%
  pairwise_similarity(chapter, word, tf_idf) %>%
  arrange(desc(similarity))


